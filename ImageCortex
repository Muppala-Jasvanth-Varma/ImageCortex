import torch
import cv2
import matplotlib.pyplot as plt
from torchvision import models, transforms
from PIL import Image

# COCO labels mapping for object detection (0-based index)
COCO_LABELS = [
    "N/A", "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",
    "traffic light", "fire hydrant", "N/A", "stop sign", "parking meter", "bench", "bird", "cat", "dog",
    "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "N/A", "backpack", "umbrella", "N/A",
    "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat",
    "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", "N/A", "wine glass", "cup",
    "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog",
    "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", "N/A", "dining table", "N/A", "toilet",
    "N/A", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", "toaster",
    "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
]

# Load pre-trained Faster R-CNN model
model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()

# Image preprocessing function
def preprocess_image(image_path):
    image = Image.open(image_path)

    # Ensure the image is in RGB format
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    transform = transforms.Compose([transforms.ToTensor()])
    return transform(image).unsqueeze(0)


# Query matching function
def query_match(predictions, query):
    matched_objects = []
    for label, score in zip(predictions['labels'], predictions['scores']):
        # Ensure the label is within the valid range of COCO labels
        if label.item() < len(COCO_LABELS):
            label_name = COCO_LABELS[label.item()]  # COCO labels are integers
            if query.lower() in label_name.lower() and score > 0.5:
                matched_objects.append((label_name, score.item()))
    return matched_objects

# Display detected objects with bounding boxes
def display_results(image_path, predictions, matched_objects):
    image = cv2.imread(image_path)
    for box, label, score in zip(predictions['boxes'], predictions['labels'], predictions['scores']):
        if score > 0.5:  # Filter based on confidence score
            x1, y1, x2, y2 = box.int().tolist()
            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)
            if label.item() < len(COCO_LABELS):
                label_name = COCO_LABELS[label.item()]
                cv2.putText(image, f"{label_name} ({score:.2f})", (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
    
    # Highlight matched objects
    for label_name, score in matched_objects:
        print(f"Found: {label_name} with confidence {score:.2f}")
    
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.axis("off")
    plt.show()

# Main function to process the image and query
def detect_objects_in_image(image_path, query):
    # Preprocess the image
    input_image = preprocess_image(image_path)
    
    # Run object detection
    with torch.no_grad():
        predictions = model(input_image)
    
    # Extract matched objects based on the query
    matched_objects = query_match(predictions[0], query)
    
    # Display the image with bounding boxes
    display_results(image_path, predictions[0], matched_objects)

# Example usage
if __name__ == "__main__":
    image_path = "W:/Computer vision/proj/hi.jpeg"
    query = "cat"  # Search query (you can change this to any object like "dog", "car", etc.)
    
    detect_objects_in_image(image_path, query)
